{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import nn\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.Dataset('train.csv')\n",
    "#train_data.to_one_hot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9839602786356664"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=256)\n",
    "pca.fit(train_data.x)\n",
    "train_x = pca.transform(train_data.x)\n",
    "np.sum(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 2.9661\n",
      "Epoch [2/200], Loss: 2.9391\n",
      "Epoch [3/200], Loss: 2.4689\n",
      "Epoch [4/200], Loss: 2.5449\n",
      "Epoch [5/200], Loss: 2.4887\n",
      "Epoch [6/200], Loss: 2.2252\n",
      "Epoch [7/200], Loss: 2.2840\n",
      "Epoch [8/200], Loss: 2.2987\n",
      "Epoch [9/200], Loss: 2.3346\n",
      "Epoch [10/200], Loss: 2.3127\n",
      "Epoch [11/200], Loss: 2.4694\n",
      "Epoch [12/200], Loss: 2.1137\n",
      "Epoch [13/200], Loss: 2.1505\n",
      "Epoch [14/200], Loss: 2.2429\n",
      "Epoch [15/200], Loss: 2.2158\n",
      "Epoch [16/200], Loss: 2.1424\n",
      "Epoch [17/200], Loss: 2.4370\n",
      "Epoch [18/200], Loss: 2.2143\n",
      "Epoch [19/200], Loss: 2.1362\n",
      "Epoch [20/200], Loss: 2.1363\n",
      "Epoch [21/200], Loss: 2.2022\n",
      "Epoch [22/200], Loss: 2.2889\n",
      "Epoch [23/200], Loss: 2.1038\n",
      "Epoch [24/200], Loss: 2.2533\n",
      "Epoch [25/200], Loss: 2.1083\n",
      "Epoch [26/200], Loss: 2.3016\n",
      "Epoch [27/200], Loss: 2.0982\n",
      "Epoch [28/200], Loss: 2.0874\n",
      "Epoch [29/200], Loss: 2.0873\n",
      "Epoch [30/200], Loss: 2.0857\n",
      "Epoch [31/200], Loss: 2.1293\n",
      "Epoch [32/200], Loss: 2.0859\n",
      "Epoch [33/200], Loss: 2.1118\n",
      "Epoch [34/200], Loss: 2.1070\n",
      "Epoch [35/200], Loss: 2.0987\n",
      "Epoch [36/200], Loss: 2.0996\n",
      "Epoch [37/200], Loss: 2.2744\n",
      "Epoch [38/200], Loss: 2.0910\n",
      "Epoch [39/200], Loss: 2.2565\n",
      "Epoch [40/200], Loss: 2.0836\n",
      "Epoch [41/200], Loss: 2.0884\n",
      "Epoch [42/200], Loss: 2.0859\n",
      "Epoch [43/200], Loss: 2.1006\n",
      "Epoch [44/200], Loss: 2.0936\n",
      "Epoch [45/200], Loss: 2.2699\n",
      "Epoch [46/200], Loss: 2.0974\n",
      "Epoch [47/200], Loss: 2.0861\n",
      "Epoch [48/200], Loss: 2.0829\n",
      "Epoch [49/200], Loss: 2.0892\n",
      "Epoch [50/200], Loss: 2.0908\n",
      "Epoch [51/200], Loss: 2.1021\n",
      "Epoch [52/200], Loss: 2.0867\n",
      "Epoch [53/200], Loss: 2.0915\n",
      "Epoch [54/200], Loss: 2.1026\n",
      "Epoch [55/200], Loss: 2.0976\n",
      "Epoch [56/200], Loss: 2.0941\n",
      "Epoch [57/200], Loss: 2.0984\n",
      "Epoch [58/200], Loss: 2.0898\n",
      "Epoch [59/200], Loss: 2.0867\n",
      "Epoch [60/200], Loss: 2.0865\n",
      "Epoch [61/200], Loss: 2.1017\n",
      "Epoch [62/200], Loss: 2.0919\n",
      "Epoch [63/200], Loss: 2.0999\n",
      "Epoch [64/200], Loss: 2.1015\n",
      "Epoch [65/200], Loss: 2.0992\n",
      "Epoch [66/200], Loss: 2.0922\n",
      "Epoch [67/200], Loss: 2.1080\n",
      "Epoch [68/200], Loss: 2.0885\n",
      "Epoch [69/200], Loss: 2.0900\n",
      "Epoch [70/200], Loss: 2.0945\n",
      "Epoch [71/200], Loss: 2.1010\n",
      "Epoch [72/200], Loss: 2.0862\n",
      "Epoch [73/200], Loss: 2.2549\n",
      "Epoch [74/200], Loss: 2.0873\n",
      "Epoch [75/200], Loss: 2.0966\n",
      "Epoch [76/200], Loss: 2.1006\n",
      "Epoch [77/200], Loss: 2.0985\n",
      "Epoch [78/200], Loss: 2.0892\n",
      "Epoch [79/200], Loss: 2.0804\n",
      "Epoch [80/200], Loss: 2.0898\n",
      "Epoch [81/200], Loss: 2.0845\n",
      "Epoch [82/200], Loss: 2.0865\n",
      "Epoch [83/200], Loss: 2.2562\n",
      "Epoch [84/200], Loss: 2.0913\n",
      "Epoch [85/200], Loss: 2.0984\n",
      "Epoch [86/200], Loss: 2.0942\n",
      "Epoch [87/200], Loss: 2.2606\n",
      "Epoch [88/200], Loss: 2.0877\n",
      "Epoch [89/200], Loss: 2.2463\n",
      "Epoch [90/200], Loss: 2.0982\n",
      "Epoch [91/200], Loss: 2.5792\n",
      "Epoch [92/200], Loss: 2.0969\n",
      "Epoch [93/200], Loss: 2.0880\n",
      "Epoch [94/200], Loss: 2.0874\n",
      "Epoch [95/200], Loss: 2.0847\n",
      "Epoch [96/200], Loss: 2.0947\n",
      "Epoch [97/200], Loss: 2.0965\n",
      "Epoch [98/200], Loss: 2.0845\n",
      "Epoch [99/200], Loss: 2.1102\n",
      "Epoch [100/200], Loss: 2.0901\n",
      "Epoch [101/200], Loss: 2.0897\n",
      "Epoch [102/200], Loss: 2.1067\n",
      "Epoch [103/200], Loss: 2.0951\n",
      "Epoch [104/200], Loss: 2.0899\n",
      "Epoch [105/200], Loss: 2.0917\n",
      "Epoch [106/200], Loss: 2.0905\n",
      "Epoch [107/200], Loss: 2.0944\n",
      "Epoch [108/200], Loss: 2.0887\n",
      "Epoch [109/200], Loss: 2.1031\n",
      "Epoch [110/200], Loss: 2.1116\n",
      "Epoch [111/200], Loss: 2.1096\n",
      "Epoch [112/200], Loss: 2.0975\n",
      "Epoch [113/200], Loss: 2.0863\n",
      "Epoch [114/200], Loss: 2.0968\n",
      "Epoch [115/200], Loss: 2.0926\n",
      "Epoch [116/200], Loss: 2.0828\n",
      "Epoch [117/200], Loss: 2.0900\n",
      "Epoch [118/200], Loss: 2.0901\n",
      "Epoch [119/200], Loss: 2.0956\n",
      "Epoch [120/200], Loss: 2.0928\n",
      "Epoch [121/200], Loss: 2.1040\n",
      "Epoch [122/200], Loss: 2.1797\n",
      "Epoch [123/200], Loss: 2.0843\n",
      "Epoch [124/200], Loss: 2.0980\n",
      "Epoch [125/200], Loss: 2.0949\n",
      "Epoch [126/200], Loss: 2.0867\n",
      "Epoch [127/200], Loss: 2.0890\n",
      "Epoch [128/200], Loss: 2.0851\n",
      "Epoch [129/200], Loss: 2.0881\n",
      "Epoch [130/200], Loss: 2.0909\n",
      "Epoch [131/200], Loss: 2.0962\n",
      "Epoch [132/200], Loss: 2.1029\n",
      "Epoch [133/200], Loss: 2.0898\n",
      "Epoch [134/200], Loss: 2.0961\n",
      "Epoch [135/200], Loss: 2.0835\n",
      "Epoch [136/200], Loss: 2.0924\n",
      "Epoch [137/200], Loss: 2.0926\n",
      "Epoch [138/200], Loss: 2.0959\n",
      "Epoch [139/200], Loss: 2.0867\n",
      "Epoch [140/200], Loss: 2.0909\n",
      "Epoch [141/200], Loss: 2.0907\n",
      "Epoch [142/200], Loss: 2.0870\n",
      "Epoch [143/200], Loss: 2.0881\n",
      "Epoch [144/200], Loss: 2.0924\n",
      "Epoch [145/200], Loss: 2.0988\n",
      "Epoch [146/200], Loss: 2.1014\n",
      "Epoch [147/200], Loss: 2.1037\n",
      "Epoch [148/200], Loss: 2.0997\n",
      "Epoch [149/200], Loss: 2.0823\n",
      "Epoch [150/200], Loss: 2.0899\n",
      "Epoch [151/200], Loss: 2.1035\n",
      "Epoch [152/200], Loss: 2.0912\n",
      "Epoch [153/200], Loss: 2.0821\n",
      "Epoch [154/200], Loss: 2.0912\n",
      "Epoch [155/200], Loss: 2.0953\n",
      "Epoch [156/200], Loss: 2.0918\n",
      "Epoch [157/200], Loss: 2.0929\n",
      "Epoch [158/200], Loss: 2.0913\n",
      "Epoch [159/200], Loss: 2.0932\n",
      "Epoch [160/200], Loss: 2.0917\n",
      "Epoch [161/200], Loss: 2.1037\n",
      "Epoch [162/200], Loss: 2.0862\n",
      "Epoch [163/200], Loss: 2.0933\n",
      "Epoch [164/200], Loss: 2.0986\n",
      "Epoch [165/200], Loss: 2.0887\n",
      "Epoch [166/200], Loss: 2.0981\n",
      "Epoch [167/200], Loss: 2.0908\n",
      "Epoch [168/200], Loss: 2.0846\n",
      "Epoch [169/200], Loss: 2.1117\n",
      "Epoch [170/200], Loss: 2.0935\n",
      "Epoch [171/200], Loss: 2.0963\n",
      "Epoch [172/200], Loss: 2.2494\n",
      "Epoch [173/200], Loss: 2.0951\n",
      "Epoch [174/200], Loss: 2.2518\n",
      "Epoch [175/200], Loss: 2.0886\n",
      "Epoch [176/200], Loss: 2.1018\n",
      "Epoch [177/200], Loss: 2.1009\n",
      "Epoch [178/200], Loss: 2.0966\n",
      "Epoch [179/200], Loss: 2.0858\n",
      "Epoch [180/200], Loss: 2.0850\n",
      "Epoch [181/200], Loss: 2.0909\n",
      "Epoch [182/200], Loss: 2.0966\n",
      "Epoch [183/200], Loss: 2.0823\n",
      "Epoch [184/200], Loss: 2.0975\n",
      "Epoch [185/200], Loss: 2.1024\n",
      "Epoch [186/200], Loss: 2.0854\n",
      "Epoch [187/200], Loss: 2.0859\n",
      "Epoch [188/200], Loss: 2.0967\n",
      "Epoch [189/200], Loss: 2.0801\n",
      "Epoch [190/200], Loss: 2.0953\n",
      "Epoch [191/200], Loss: 2.0890\n",
      "Epoch [192/200], Loss: 2.0868\n",
      "Epoch [193/200], Loss: 2.1000\n",
      "Epoch [194/200], Loss: 2.0973\n",
      "Epoch [195/200], Loss: 2.0852\n",
      "Epoch [196/200], Loss: 2.1004\n",
      "Epoch [197/200], Loss: 2.0998\n",
      "Epoch [198/200], Loss: 2.0936\n",
      "Epoch [199/200], Loss: 2.0971\n",
      "Epoch [200/200], Loss: 2.0868\n",
      "tensor(0.8033)\n"
     ]
    }
   ],
   "source": [
    "path = 'model.pth'\n",
    "max_acc = nn.train(train_x, train_data.y, validation_split=0.1, epochs=200,path=path,weight_decay=10e-4/2)\n",
    "model = nn.Net(train_x.shape[1])\n",
    "model.load_state_dict(torch.load(path))\n",
    "print(max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x,test_x,train_y,test_y = train_test_split(train_x,train_data.y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.Dataset('test.csv')\n",
    "test_data_x = pca.transform(test_data.x)\n",
    "pred = model(torch.from_numpy(test_data_x.astype(np.float32))).detach().numpy()\n",
    "pred = np.argmax(pred,axis=1)\n",
    "data.write_to_csv('submission.csv',pred,train_data.get_cat_to_label())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
