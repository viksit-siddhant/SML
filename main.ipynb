{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor as LOF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "import nn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.Dataset('train.csv')\n",
    "#train_data.to_one_hot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LOF(n_neighbors=5)\n",
    "outlier_labels = lof.fit_predict(train_data.x)\n",
    "train_data.x = train_data.x[outlier_labels == 1]\n",
    "train_data.y = train_data.y[outlier_labels == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_data.x = scaler.fit_transform(train_data.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9841102719104073"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=256)\n",
    "pca.fit(train_data.x)\n",
    "train_data.x = pca.transform(train_data.x)\n",
    "np.sum(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(train_data.x,train_data.y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1084, 256) (1084,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape,train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 2.9582\n",
      "Epoch [2/200], Loss: 2.9070\n",
      "Epoch [3/200], Loss: 2.8269\n",
      "Epoch [4/200], Loss: 2.7325\n",
      "Epoch [5/200], Loss: 2.6879\n",
      "Epoch [6/200], Loss: 2.5820\n",
      "Epoch [7/200], Loss: 2.5175\n",
      "Epoch [8/200], Loss: 2.5303\n",
      "Epoch [9/200], Loss: 2.4889\n",
      "Epoch [10/200], Loss: 2.4579\n",
      "Epoch [11/200], Loss: 2.3553\n",
      "Epoch [12/200], Loss: 2.2711\n",
      "Epoch [13/200], Loss: 2.2685\n",
      "Epoch [14/200], Loss: 2.2977\n",
      "Epoch [15/200], Loss: 2.3081\n",
      "Epoch [16/200], Loss: 2.2920\n",
      "Epoch [17/200], Loss: 2.2687\n",
      "Epoch [18/200], Loss: 2.2678\n",
      "Epoch [19/200], Loss: 2.2859\n",
      "Epoch [20/200], Loss: 2.2666\n",
      "Epoch [21/200], Loss: 2.1990\n",
      "Epoch [22/200], Loss: 2.2009\n",
      "Epoch [23/200], Loss: 2.2452\n",
      "Epoch [24/200], Loss: 2.1426\n",
      "Epoch [25/200], Loss: 2.1557\n",
      "Epoch [26/200], Loss: 2.2157\n",
      "Epoch [27/200], Loss: 2.1611\n",
      "Epoch [28/200], Loss: 2.1431\n",
      "Epoch [29/200], Loss: 2.1851\n",
      "Epoch [30/200], Loss: 2.1279\n",
      "Epoch [31/200], Loss: 2.1374\n",
      "Epoch [32/200], Loss: 2.1802\n",
      "Epoch [33/200], Loss: 2.1710\n",
      "Epoch [34/200], Loss: 2.1093\n",
      "Epoch [35/200], Loss: 2.1290\n",
      "Epoch [36/200], Loss: 2.2006\n",
      "Epoch [37/200], Loss: 2.1004\n",
      "Epoch [38/200], Loss: 2.0966\n",
      "Epoch [39/200], Loss: 2.1000\n",
      "Epoch [40/200], Loss: 2.1292\n",
      "Epoch [41/200], Loss: 2.1359\n",
      "Epoch [42/200], Loss: 2.1373\n",
      "Epoch [43/200], Loss: 2.1240\n",
      "Epoch [44/200], Loss: 2.1658\n",
      "Epoch [45/200], Loss: 2.0989\n",
      "Epoch [46/200], Loss: 2.1278\n",
      "Epoch [47/200], Loss: 2.1151\n",
      "Epoch [48/200], Loss: 2.1041\n",
      "Epoch [49/200], Loss: 2.0888\n",
      "Epoch [50/200], Loss: 2.1028\n",
      "Epoch [51/200], Loss: 2.0940\n",
      "Epoch [52/200], Loss: 2.0903\n",
      "Epoch [53/200], Loss: 2.1392\n",
      "Epoch [54/200], Loss: 2.1263\n",
      "Epoch [55/200], Loss: 2.0910\n",
      "Epoch [56/200], Loss: 2.0891\n",
      "Epoch [57/200], Loss: 2.0849\n",
      "Epoch [58/200], Loss: 2.0895\n",
      "Epoch [59/200], Loss: 2.1557\n",
      "Epoch [60/200], Loss: 2.1249\n",
      "Epoch [61/200], Loss: 2.1246\n",
      "Epoch [62/200], Loss: 2.0869\n",
      "Epoch [63/200], Loss: 2.1055\n",
      "Epoch [64/200], Loss: 2.1204\n",
      "Epoch [65/200], Loss: 2.0871\n",
      "Epoch [66/200], Loss: 2.0859\n",
      "Epoch [67/200], Loss: 2.1224\n",
      "Epoch [68/200], Loss: 2.0875\n",
      "Epoch [69/200], Loss: 2.0875\n",
      "Epoch [70/200], Loss: 2.0845\n",
      "Epoch [71/200], Loss: 2.0861\n",
      "Epoch [72/200], Loss: 2.1206\n",
      "Epoch [73/200], Loss: 2.0870\n",
      "Epoch [74/200], Loss: 2.0837\n",
      "Epoch [75/200], Loss: 2.0822\n",
      "Epoch [76/200], Loss: 2.0821\n",
      "Epoch [77/200], Loss: 2.0850\n",
      "Epoch [78/200], Loss: 2.0842\n",
      "Epoch [79/200], Loss: 2.0860\n",
      "Epoch [80/200], Loss: 2.0831\n",
      "Epoch [81/200], Loss: 2.0827\n",
      "Epoch [82/200], Loss: 2.0829\n",
      "Epoch [83/200], Loss: 2.1170\n",
      "Epoch [84/200], Loss: 2.0851\n",
      "Epoch [85/200], Loss: 2.0874\n",
      "Epoch [86/200], Loss: 2.0849\n",
      "Epoch [87/200], Loss: 2.0829\n",
      "Epoch [88/200], Loss: 2.0811\n",
      "Epoch [89/200], Loss: 2.0843\n",
      "Epoch [90/200], Loss: 2.1177\n",
      "Epoch [91/200], Loss: 2.0903\n",
      "Epoch [92/200], Loss: 2.0866\n",
      "Epoch [93/200], Loss: 2.0829\n",
      "Epoch [94/200], Loss: 2.1169\n",
      "Epoch [95/200], Loss: 2.1220\n",
      "Epoch [96/200], Loss: 2.0823\n",
      "Epoch [97/200], Loss: 2.0825\n",
      "Epoch [98/200], Loss: 2.0812\n",
      "Epoch [99/200], Loss: 2.0822\n",
      "Epoch [100/200], Loss: 2.0810\n",
      "Epoch [101/200], Loss: 2.0802\n",
      "Epoch [102/200], Loss: 2.0805\n",
      "Epoch [103/200], Loss: 2.1202\n",
      "Epoch [104/200], Loss: 2.0800\n",
      "Epoch [105/200], Loss: 2.0863\n",
      "Epoch [106/200], Loss: 2.0838\n",
      "Epoch [107/200], Loss: 2.0802\n",
      "Epoch [108/200], Loss: 2.1201\n",
      "Epoch [109/200], Loss: 2.0811\n",
      "Epoch [110/200], Loss: 2.0810\n",
      "Epoch [111/200], Loss: 2.0820\n",
      "Epoch [112/200], Loss: 2.1188\n",
      "Epoch [113/200], Loss: 2.0823\n",
      "Epoch [114/200], Loss: 2.0803\n",
      "Epoch [115/200], Loss: 2.0803\n",
      "Epoch [116/200], Loss: 2.0842\n",
      "Epoch [117/200], Loss: 2.0798\n",
      "Epoch [118/200], Loss: 2.1182\n",
      "Epoch [119/200], Loss: 2.0803\n",
      "Epoch [120/200], Loss: 2.0806\n",
      "Epoch [121/200], Loss: 2.0798\n",
      "Epoch [122/200], Loss: 2.0828\n",
      "Epoch [123/200], Loss: 2.0808\n",
      "Epoch [124/200], Loss: 2.0797\n",
      "Epoch [125/200], Loss: 2.0805\n",
      "Epoch [126/200], Loss: 2.0797\n",
      "Epoch [127/200], Loss: 2.0855\n",
      "Epoch [128/200], Loss: 2.1132\n",
      "Epoch [129/200], Loss: 2.0821\n",
      "Epoch [130/200], Loss: 2.1149\n",
      "Epoch [131/200], Loss: 2.0808\n",
      "Epoch [132/200], Loss: 2.0834\n",
      "Epoch [133/200], Loss: 2.0813\n",
      "Epoch [134/200], Loss: 2.0838\n",
      "Epoch [135/200], Loss: 2.0794\n",
      "Epoch [136/200], Loss: 2.0802\n",
      "Epoch [137/200], Loss: 2.0811\n",
      "Epoch [138/200], Loss: 2.0797\n",
      "Epoch [139/200], Loss: 2.0806\n",
      "Epoch [140/200], Loss: 2.0801\n",
      "Epoch [141/200], Loss: 2.0809\n",
      "Epoch [142/200], Loss: 2.0793\n",
      "Epoch [143/200], Loss: 2.0796\n",
      "Epoch [144/200], Loss: 2.0796\n",
      "Epoch [145/200], Loss: 2.0794\n",
      "Epoch [146/200], Loss: 2.0797\n",
      "Epoch [147/200], Loss: 2.0798\n",
      "Epoch [148/200], Loss: 2.1150\n",
      "Epoch [149/200], Loss: 2.0794\n",
      "Epoch [150/200], Loss: 2.0808\n",
      "Epoch [151/200], Loss: 2.1139\n",
      "Epoch [152/200], Loss: 2.0793\n",
      "Epoch [153/200], Loss: 2.0799\n",
      "Epoch [154/200], Loss: 2.0796\n",
      "Epoch [155/200], Loss: 2.0793\n",
      "Epoch [156/200], Loss: 2.0805\n",
      "Epoch [157/200], Loss: 2.1141\n",
      "Epoch [158/200], Loss: 2.0810\n",
      "Epoch [159/200], Loss: 2.0801\n",
      "Epoch [160/200], Loss: 2.0792\n",
      "Epoch [161/200], Loss: 2.0793\n",
      "Epoch [162/200], Loss: 2.0799\n",
      "Epoch [163/200], Loss: 2.0794\n",
      "Epoch [164/200], Loss: 2.0793\n",
      "Epoch [165/200], Loss: 2.0794\n",
      "Epoch [166/200], Loss: 2.0794\n",
      "Epoch [167/200], Loss: 2.0794\n",
      "Epoch [168/200], Loss: 2.1133\n",
      "Epoch [169/200], Loss: 2.0793\n",
      "Epoch [170/200], Loss: 2.0790\n",
      "Epoch [171/200], Loss: 2.0793\n",
      "Epoch [172/200], Loss: 2.0797\n",
      "Epoch [173/200], Loss: 2.0799\n",
      "Epoch [174/200], Loss: 2.0794\n",
      "Epoch [175/200], Loss: 2.0799\n",
      "Epoch [176/200], Loss: 2.0805\n",
      "Epoch [177/200], Loss: 2.0833\n",
      "Epoch [178/200], Loss: 2.0788\n",
      "Epoch [179/200], Loss: 2.0793\n",
      "Epoch [180/200], Loss: 2.0789\n",
      "Epoch [181/200], Loss: 2.0792\n",
      "Epoch [182/200], Loss: 2.0790\n",
      "Epoch [183/200], Loss: 2.0791\n",
      "Epoch [184/200], Loss: 2.1133\n",
      "Epoch [185/200], Loss: 2.0794\n",
      "Epoch [186/200], Loss: 2.0788\n",
      "Epoch [187/200], Loss: 2.0793\n",
      "Epoch [188/200], Loss: 2.0788\n",
      "Epoch [189/200], Loss: 2.1136\n",
      "Epoch [190/200], Loss: 2.0793\n",
      "Epoch [191/200], Loss: 2.0790\n",
      "Epoch [192/200], Loss: 2.0792\n",
      "Epoch [193/200], Loss: 2.0795\n",
      "Epoch [194/200], Loss: 2.0794\n",
      "Epoch [195/200], Loss: 2.0791\n",
      "Epoch [196/200], Loss: 2.0794\n",
      "Epoch [197/200], Loss: 2.0788\n",
      "Epoch [198/200], Loss: 2.0788\n",
      "Epoch [199/200], Loss: 2.0788\n",
      "Epoch [200/200], Loss: 2.0785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7769)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.train(train_data.x,train_data.y,validation_split=0.1,epochs=200,weight_decay=1.5*10e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71900826446281"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(train_x,train_y)\n",
    "tran_x = lda.transform(train_x)\n",
    "trantest_x = lda.transform(test_x)\n",
    "\n",
    "clf = RFC(min_samples_split=8,bootstrap=False,criterion='entropy',max_depth=20,max_features='sqrt',min_samples_leaf=1,n_estimators=1000)\n",
    "\n",
    "clf.fit(tran_x,train_y)\n",
    "print(clf.score(tran_x,train_y))\n",
    "clf.score(trantest_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "m = nn.Net(train_data.x.shape[1])\n",
    "m.load_state_dict(torch.load('model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(torch.from_numpy(test_x).float()).argmax(1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6513184843830006\n",
      "{'bootstrap': False, 'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 8}\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.Dataset('test.csv')\n",
    "test_x = pca.transform(test_data.x)\n",
    "pred = m(torch.from_numpy(test_x).float()).argmax(1).numpy()\n",
    "data.write_to_csv('submission.csv',pred,train_data.get_cat_to_label())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
